#' note: this is a small adapation from the original fslSCEPTICModel to avoid use of the clockfit objects and to move to the
#' simpler build_design_matrix approach and the use of the trial_statistics csv files from vba_fmri
#'
#' @param d_obj a single-subject design matrix object generated by build_design_matrix
#' @param gpa a gpa (glm_pipeline_arguments) object generated by setup_glm_pipeline
#' @param model_name a string indicating the model name within \code{gpa} to setup
#' @param mr_files an optional character vector of NIfTI filenames used in l1 analysis
#'
#' @importFrom checkmate assert_class assert_string assert_character assert_file_exists
#' @importFrom lgr get_logger
#' @importFrom parallel clusterApply stopCluster makeForkCluster
#' @author Michael Hallquist
#' @export
#' 
fsl_l1_model <- function(d_obj, gpa, model_name=NULL, mr_files=NULL) {
  checkmate::assert_class(gpa, "glm_pipeline_arguments")
  checkmate::assert_class(d_obj, "bdm")
  checkmate::assert_string(model_name) #single string
  checkmate::assert_character(mr_files, null.ok=FALSE)
  checkmate::assert_file_exists(mr_files) #all exist
  
  stopifnot(model_name %in% names(gpa$l1_models$models))
  
  lg <- lgr::get_logger("glm_pipeline/l1_setup")

  #TODO use system.file to read from R package installation dir
  fsfTemplate <- readLines(file.path(gpa$pipeline_home, "inst", "feat_lvl1_nparam_template.fsf"))  
  
  #note: normalizePath will fail to evaluate properly if directory does not exist
  fsl_run_output_dir <- file.path(normalizePath(file.path(dirname(mr_files[1L]), "..")), outdir)

  if (file.exists(fsl_run_output_dir) && isFALSE(gpa$force_l1_creation)) {
    lg$info("%s exists. Skipping l1 fsf setup in fsl_l1_model().", fsl_run_output_dir)
    return(0)
  }

  lg$info("Create l1 fsl_run_output_dir: %s", fsl_run_output_dir)
  dir.create(fsl_run_output_dir, showWarnings=FALSE) #one directory up from a given run
  timingdir <- file.path(fsl_run_output_dir, "run_timing")
  
  allFeatFiles <- list()

  if (!is.null(d_obj$run_4d_files)) {
    lg$debug("Using internal NIfTI files (run_4d_files) within d_obj for Feat level 1 setup")
    mr_files <- d_obj$run_4d_files
  }

  stopifnot(length(mr_files) == length(d_obj$run_volumes)) #need these to align
  
  #FSL computes first-level models on individual runs
  for (r in seq_along(mr_files)) {
    stopifnot(file.exists(file.path(dirname(mr_files[r]), "motion.par"))) #can't find motion parameters
    
    nvol <- oro.nifti::readNIfTI(mr_files[r], read_data=FALSE)$dim_[5L]

    mregressors <- NULL #start with NULL

    if (spikeregressors) { #incorporate spike regressors if requested (not used in conventional AROMA)
      censorfile <- file.path(dirname(mr_files[r]), "motion_info", "fd_0.9.mat")
      if (file.exists(censorfile) && file.info(censorfile)$size > 0) {
        censor <- read.table(censorfile, header=FALSE)
        censor <- censor[(1+drop_volumes):runlengths[r],,drop=FALSE] #need no drop here in case there is just a single volume to censor
        #if the spikes fall outside of the rows selected above, we will obtain an all-zero column. remove these
        censor <- censor[,sapply(censor, sum) > 0,drop=FALSE]
        if (ncol(censor) == 0L) { censor <- NULL } #no volumes to censor within valid timepoints
        mregressors <- censor
      }
    }
    
    ##add CSF and WM regressors (with their derivatives)
    nuisancefile <- file.path(dirname(mr_files[r]), "nuisance_regressors.txt")
    if (file.exists(nuisancefile)) {
      nuisance <- read.table(nuisancefile, header=FALSE)
      nuisance <- nuisance[(1+drop_volumes):runlengths[r],,drop=FALSE]
      nuisance <- as.data.frame(lapply(nuisance, function(col) { col - mean(col) })) #demean
      #cat("about to cbind with nuisance\n")
      #print(str(mregressors))
      #print(str(nuisance))
      if (!is.null(mregressors)) { mregressors <- cbind(mregressors, nuisance) #note that in R 3.3.0, cbind with NULL or c() is no problem...
      } else { mregressors <- nuisance }
    }
    
    motfile <- file.path(fsl_run_output_dir, paste0("run", runnum, "_confounds.txt"))
    write.table(mregressors, file=motfile, col.names=FALSE, row.names=FALSE)

    #search and replace within fsf file for appropriate sections
    # .OUTPUTDIR. is the feat output location
    # .NVOL. is the number of volumes in the run
    # .FUNCTIONAL. is the fmri data to process (sans extension)
    # .CONFOUNDS. is the confounds file for GLM
    # .TR. is the sequence TR in seconds
    
    thisTemplate <- fsfTemplate
    thisTemplate <- gsub(".OUTPUTDIR.", file.path(fsl_run_output_dir, paste0("FEAT_LVL1_run", runnum)), thisTemplate, fixed=TRUE)
    thisTemplate <- gsub(".NVOL.", nvol, thisTemplate, fixed=TRUE)
    thisTemplate <- gsub(".FUNCTIONAL.", gsub(".nii(.gz)*$", "", mr_files[r]), thisTemplate, fixed=TRUE)
    thisTemplate <- gsub(".CONFOUNDS.", motfile, thisTemplate, fixed=TRUE)
    thisTemplate <- gsub(".TR.", d_obj$tr, thisTemplate, fixed=TRUE)

    if (isTRUE(gpa$use_preconvolve)) {
      lg$info("Using preconvolved regressors in Feat level 1 analysis")
      
      #generate ev syntax
      dmat <- d$design_convolved[[paste0("run", runnum)]] %>% select(-matches("base\\d+")) #drop baseline columns
      regressors <- as.list(names(dmat))

      #add common ingredients for preconvolved regressors
      regressors <- lapply(regressors, function(x) {
        list(name=x, waveform="custom_1", convolution="none",
          tempfilt=1, timing_file=file.path(timingdir, paste0("run", runnum, "_", x, ".1D")))
      })

      lg$debug("dependlab::generate_fsf_lvl1_ev_syntax")
      ev_syn <- dependlab::generate_fsf_lvl1_ev_syntax(regressors)

      #creation of l1 contrast matrices, including the diagonal contrasts, now abstracted to finalize_pipeline_configuration.R
      #thus, l1_contrasts is already a contrast matrix ready to be passed to the generate_fsf_contrast_syntax function
      cmat_syn <- dependlab::generate_fsf_contrast_syntax(l1_contrasts)

      #combine all syntax
      thisTemplate <- c(thisTemplate, ev_syn, cmat_syn)
    } else {
      lg$error("No support for FSL-internal convolved yet")
      stop("cannot use FSL internal")
    }
    
    featFile <- file.path(fsl_run_output_dir, paste0("FEAT_LVL1_run", runnum, ".fsf"))
    if (file.exists(featFile) && force==FALSE) { next } #skip re-creation of FSF and do not run below unless force==TRUE 
    cat(thisTemplate, file=featFile, sep="\n")
    
    allFeatFiles[[r]] <- featFile
  }   

  #if execute_feat is TRUE, execute feat on each fsf files at this stage, using an 8-node socket cluster (since we have 8 runs)
  #if execute_feat is FALSE, just create the fsf files but don't execute the analysis
  if (isTRUE(execute_feat)) {
    require(parallel)
    cl_fork <- makeForkCluster(nnodes=8)
    runfeat <- function(fsf) {
      runname <- basename(fsf)
      runFSLCommand(paste("feat", fsf), stdout=file.path(dirname(fsf), paste0("feat_stdout_", runname)),
        stderr=file.path(dirname(fsf), paste0("feat_stderr_", runname)))
    }
    clusterApply(cl_fork, allFeatFiles, runfeat)
    stopCluster(cl_fork)
  }
  
}
